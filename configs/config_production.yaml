# Unified Configuration for ModularModel
# This file combines model architectures and training configurations

# Model Architecture Configurations
model_configs:
  model_config_32.7B:
    decoder_config:
      hidden_size: 5120
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 40960
      num_hidden_layers: 64
      num_attention_heads: 64
      intermediate_size: 25600
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_15.7B:
    decoder_config:
      hidden_size: 5120
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 40960
      num_hidden_layers: 40
      num_attention_heads: 40
      intermediate_size: 17408
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_8.5B:
    decoder_config:
      hidden_size: 4096
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 40960
      num_hidden_layers: 36
      num_attention_heads: 32
      intermediate_size: 12288
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-05
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_4.0B:
    decoder_config:
      attention_bias: false
      attention_dropout: 0.0
      hidden_act: "silu"
      hidden_size: 2560
      initializer_range: 0.02
      intermediate_size: 9728
      max_position_embeddings: 40960
      max_window_layers: 36
      num_attention_heads: 32
      num_hidden_layers: 36
      num_key_value_heads: 8
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_1.8B:
    decoder_config:
      attention_bias: false
      attention_dropout: 0.0
      hidden_act: "silu"
      hidden_size: 2048
      initializer_range: 0.02
      intermediate_size: 6144
      max_position_embeddings: 40960
      max_window_layers: 28
      num_attention_heads: 16
      num_hidden_layers: 28
      num_key_value_heads: 8
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_1B:
    decoder_config:
      attention_bias: false
      attention_dropout: 0.0
      hidden_act: "silu"
      hidden_size: 1536
      initializer_range: 0.02
      intermediate_size: 3840
      max_position_embeddings: 40960
      max_window_layers: 28
      num_attention_heads: 24
      num_hidden_layers: 28
      num_key_value_heads: 8
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_243M:
    decoder_config:
      hidden_size: 1024
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 512
      num_hidden_layers: 12
      num_attention_heads: 16
      intermediate_size: 1024
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-05
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_89M:
    decoder_config:
      hidden_size: 512
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 256
      num_hidden_layers: 6
      num_attention_heads: 8
      intermediate_size: 512
      num_key_value_heads: 4
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-05
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

# Training Stage Configurations
training_stages:
  # Stage 1: Pre-training
  stage1:
    model:
      model_config_key: "model_config_1.8B"
      tokenizer_path: "Qwen/Qwen3-Coder-30B-A3B-Instruct"
      use_flash_attention: true
      disable_flash_attention: false
      
      # Model architecture options (inherited from model_config_1.8B)
      tie_weights: false            # Tie embedding and LM head weights
      freeze_embedder_decoder: true # Freeze embedder decoder
      embedder_checkpoint_path: null # Path to embedder checkpoint

    training:
      # Training backend selection (for production mode)
      training_backend: "lightning"  # Options: "lightning" (PyTorch Lightning) or "megatron" (NeMo Megatron)
      
      # Training duration - optimized for 10M samples with 1B model
      epochs: 1                     # Single epoch for large dataset (10M samples)
      batch_size: 15                # Increased to compensate for reduced gradient accumulation
      sequence_length: 1024         # Reduced from 2048 to reduce memory usage
      learning_rate: 4e-5           # Keep current learning rate
      max_grad_norm: 0.8            # Keep current gradient clipping
      gradient_accumulation_steps: 2 # Reduced from 4 to minimize overhead
      gradient_checkpointing: true   # Essential for memory efficiency
      mixed_precision: "bf16-mixed"       # Use bf16 for GH200 optimization
      
      # Step-based checkpointing and validation (optimized for large dataset)
      checkpointing:
        save_every_n_steps: 500    # Less frequent saves to avoid FSDP issues
        save_top_k: 3               # Keep more checkpoints for large training
        monitor: "val_loss"         # Metric to monitor for best checkpoints
        mode: "min"                 # Monitor mode (min/max)
        filename: "checkpoint-{step:06d}-{val_loss:.4f}"  # Checkpoint filename
        auto_insert_metric_name: false  # Don't insert metric name in filename
        
        # Resume training settings
        resume_from_checkpoint: null  # Path to checkpoint to resume from (null = auto-detect, set to "none" to disable)
        resume_ignore_mismatched_sizes: true  # Ignore size mismatches when resuming
        
      # Training monitoring and control (optimized for large dataset)
      log_every_n_steps: 100        # Reduced logging frequency to minimize overhead

      val_check_interval_steps: 50000   # Reduce validation frequency to minimize overhead
      
      # Early stopping (disabled for large dataset training)
      patience: 10                  # Higher patience for large dataset training
      
      # Gradient optimization
      gradient_clip_val: 0.8        # Tighter gradient clipping for 1B model
      gradient_clip_algorithm: "value"  # Gradient clipping algorithm (compatible with FSDP)
    
    # Distributed training configuration (optimized for GH200)
    distributed:
      strategy: "fsdp"  # Use FSDP for large model training on GH200
      num_nodes: 1
      devices: "auto"  # Can be "auto", number, or list of device IDs
      sync_batchnorm: false
      
      # FSDP specific configuration (optimized for 1B model)
      fsdp:
        enabled: true                # Enable FSDP for memory efficiency
        cpu_offload: false           # Disable CPU offload for better performance
        sharding_strategy: "FULL_SHARD"  # Full sharding for maximum memory efficiency
        backward_prefetch: "BACKWARD_PRE"  # Optimize backward pass
        forward_prefetch: true       # Enable forward prefetch for better performance
        limit_all_gathers: true      # Reduce communication overhead
        activation_checkpointing: true   # Re-enable to reduce memory usage
        use_orig_params: true  # Set to true for single GPU FSDP compatibility
    
    # Optimizer configuration (optimized for 1B model)
    optimizer:
      type: "AdamW"
      weight_decay: 0.05             # Lower weight decay for 1B model
      betas: [0.9, 0.98]             # More stable betas for 1B model
      eps: 1e-6                      # Slightly higher eps for numerical stability
    
    # Learning rate scheduler configuration (optimized for 1B model)
    scheduler:
      type: "CosineAnnealingLR"      # Cosine annealing for better convergence
      T_max: 80000                   # Adjusted total steps for 1B model
      eta_min: 1e-7                  # Minimum learning rate
      warmup_steps: 5000             # Longer warmup for large dataset
      interval: "step"
      frequency: 1
    
    # Dataset configuration (NeMo HFDatasetDataModule with ds_processing pipeline)
    data:
      # Processing configuration for ds_processing pipeline
      processing:
        max_tokens_per_sample: 16384           # Maximum tokens per document (allows full documents with smart chunking)
        min_tokens_per_sample: 50              # Minimum tokens per training sample
        overlap_tokens: 200                    # Overlap between consecutive chunks for better training
        shuffle_datasets: true                 # Shuffle datasets before mixing
        seed: 42                               # Random seed for reproducibility
        total_samples: 20000000                 # Total samples for all datasets combined (code calculates per-dataset based on percentages) - Reduced for faster testing
        use_processed_datasets: true           # Set to true to use preprocessed datasets (much faster!)
        save_processed_datasets: true          # Save processed datasets for future use
        test_size_samples: false               # If true, use only 4096 samples for testing; if false, use full dataset
      
      # Output configuration for ds_processing pipeline
      output:
        processed_data_dir: "data/processed"   # Directory for processed HuggingFace datasets
        cache_dir: "/tmp/hf_cache"             # Use temporary cache directory
        save_format: "hf"                      # HuggingFace format for processed data
      
      # Stage 1: Pre-training datasets (Next Token Prediction)
      # Based on optimal dataset mixing research (50-30-20 mix):
      # - 50% finePDFs (high-quality textbook-style PDFs)
      # - 30% DCLM-baseline (filtered, diverse web content)
      # - 20% FineWeb-Edu (curated educational web resources)
      # Reference: https://huggingface.co/blog/codelion/optimal-dataset-mixing
      # This mix achieves 90%+ performance with 1/10th the data compared to naive approaches
      pretraining_datasets:
        # ===== OPTIMAL MIX DATASETS (from research) =====
        # High-quality textbook-style PDFs (50% recommended)
        "codelion/finepdfs-1B":
          percentage: 33.0                     # High-quality textbook-style PDFs (primary source for 50% finePDFs target)
          subset: null                         # Full 1B token dataset
        
        # Filtered, diverse web content (30% recommended)
        "codelion/dclm-baseline-1B":
          percentage: 19.0                     # DCLM-baseline: filtered, diverse web content (primary source for 30% DCLM target)
          subset: null                         # Full 1B token dataset
        
        # Curated educational web resources (20% recommended)
        "codelion/fineweb-edu-1B":
          percentage: 11.0                     # FineWeb-Edu: curated educational web resources (primary source for 20% FineWeb-Edu target)
          subset: null                         # Full 1B token dataset
        
        # ===== EXISTING DATASETS (kept for diversity and additional coverage) =====
        # Hugging Face pretraining datasets with percentage allocation
        "HuggingFaceFW/fineweb":
          percentage: 5.0                      # High-quality web content dataset (FineWeb-Edu category)
          subset: "sample-10BT"               # 10 billion token sample
        
        # Math and reasoning datasets (finePDFs-like educational content)
        "HuggingFaceTB/finemath":
          percentage: 7.0                      # Math content subset (finePDFs category - educational/math content)
          subset: "finemath-3plus"             # Math content subset
        
        # Curated content (DCLM-baseline variant)
        "mlfoundations/dclm-baseline-1.0":
          percentage: 9.0                      # DCLM-baseline variant: curated content (DCLM-baseline category)
          subset: null                         # 187k tokens - Curated content
        
        # Synthetic educational content (finePDFs-like)
        "HuggingFaceTB/cosmopedia":
          percentage: 6.0                      # Synthetic textbooks subset (finePDFs category - synthetic educational)
          subset: "auto_math_text"             # Synthetic textbooks subset

        # FineWeb-Edu variant (nanochat dataset)
        "karpathy/fineweb-edu-100b-shuffle":
          percentage: 3.0                      # FineWeb-Edu variant: educational web content (FineWeb-Edu category)
          subset: null                         # 100 billion token sample
        
        # ===== CODE DATASETS (high-quality structured content) =====
        # Large-scale curated source code dataset from GitHub
        # Includes code question-answer pairs in 11 programming languages
        # Note: Dataset may use 'code', 'content', or 'text' fields - dataset loader handles all
        "nvidia/Nemotron-Pretraining-Code-v1":
          percentage: 7.0                      # High-quality code dataset (finePDFs category - structured/educational)
          subset: Synthetic_Code                         # Full dataset (174.9B tokens of synthetic code + curated GitHub code)
          # Note: This dataset requires accepting NVIDIA Data Agreement for Model Training
          # The dataset includes both raw code and code Q&A pairs
          # Dataset loader will extract text from 'code', 'content', or 'text' fields automatically
          # Reference: https://huggingface.co/datasets/nvidia/Nemotron-Pretraining-Code-v1
        
        # Total: 100.0% (ensures proper allocation)
        # Breakdown by category (aligned with optimal 50-30-20 mix):
        # - finePDFs category: 33% (codelion/finepdfs-1B) + 7% (finemath) + 6% (cosmopedia) + 7% (Nemotron-Code) = 53%
        # - DCLM-baseline category: 19% (codelion/dclm-baseline-1B) + 9% (mlfoundations/dclm-baseline-1.0) = 28%
        # - FineWeb-Edu category: 11% (codelion/fineweb-edu-1B) + 5% (fineweb) + 3% (karpathy/fineweb-edu) = 19%
        # Note: Code dataset (7%) added to finePDFs category as structured/educational content
      
      # Data loading performance options (optimized for GH200 with 72 CPU cores)
      num_workers: 64                # Reduced from 16 to minimize overhead and contention
      pin_memory: true              # Faster GPU transfer
      persistent_workers: true      # Keep workers alive between batches
      prefetch_factor: 2            # Reduced from 4 to minimize memory overhead

    checkpoint_dir: "/home/sureshm/nemoACCfiy/outputs/checkpoints/stage1_v2"
    model_output_dir: "/home/sureshm/nemoACCfiy/outputs/models_v2"
    model_output_name: "stage1_model_v2.pth"

    wandb:
      use_wandb: false
      project: "accfiy-stage1-v2"
      entity: null
      run_name: null
      tags: ["stage1", "pretraining", "v2"]
      group: null
      notes: null

    logging:
      debug: false
      log_file: null
      enable_profiling: true
      profiler_log_dir: "./profiler_logs/stage1_v2"
      save_steps: 10000
      validation_steps: 10000
    
    # Environment and reproducibility
    environment:
      seed: 42                      # Random seed for reproducibility
      deterministic: false          # Deterministic training (slower but reproducible)
      benchmark: true               # CUDNN benchmark for better performance

  # Stage 1 Instruction SFT: Instruction Fine-tuning (decoder-only, after NTP)
  stage1_inst_SFT:
    model:
      model_config_key: "model_config_1.8B"
      tokenizer_path: "Qwen/Qwen3-Coder-30B-A3B-Instruct"
      use_flash_attention: true
      disable_flash_attention: false
      
      # Model architecture options (inherited from model_config_1.8B)
      tie_weights: false            # Tie embedding and LM head weights
      freeze_embedder_decoder: true # Freeze embedder decoder
      embedder_checkpoint_path: null # Path to embedder checkpoint

    training:
      # Training backend selection (for production mode)
      training_backend: "lightning"  # Options: "lightning" (PyTorch Lightning) or "megatron" (NeMo Megatron)
      
      # Training duration - optimized for instruction tuning
      epochs: 3                     # Multiple epochs for instruction tuning
      batch_size: 16                # Batch size for instruction SFT
      sequence_length: 2048         # Sequence length for instruction tuning
      learning_rate: 1e-5           # Lower learning rate for fine-tuning
      max_grad_norm: 0.8            # Gradient clipping
      gradient_accumulation_steps: 2 # Gradient accumulation steps
      gradient_checkpointing: true   # Essential for memory efficiency
      mixed_precision: "bf16-mixed" # Use bf16 for GH200 optimization
      
      # Step-based checkpointing and validation
      checkpointing:
        save_every_n_steps: 1000    # Save checkpoints every 1000 steps
        save_top_k: 3               # Keep top 3 checkpoints
        monitor: "val_loss"         # Metric to monitor for best checkpoints
        mode: "min"                 # Monitor mode (min/max)
        filename: "stage1_inst_sft_checkpoint-{step:06d}-{val_loss:.4f}"  # Checkpoint filename
        auto_insert_metric_name: false  # Don't insert metric name in filename
        
        # Resume training settings
        resume_from_checkpoint: null  # Path to checkpoint to resume from
        resume_ignore_mismatched_sizes: true  # Ignore size mismatches when resuming
        
      # Training monitoring and control
      log_every_n_steps: 100        # Log every 100 steps
      val_check_interval_steps: 1000   # Validate every 1000 steps
      
      # Early stopping
      patience: 5                    # Early stopping patience
      
      # Gradient optimization
      gradient_clip_val: 0.8         # Gradient clipping value
      gradient_clip_algorithm: "value"  # Gradient clipping algorithm (compatible with FSDP)
    
    # Distributed training configuration (optimized for GH200)
    distributed:
      strategy: "fsdp"  # Use FSDP for large model training on GH200
      num_nodes: 1
      devices: "auto"  # Can be "auto", number, or list of device IDs
      sync_batchnorm: false
      
      # FSDP specific configuration
      fsdp:
        enabled: true                # Enable FSDP for memory efficiency
        cpu_offload: false           # Disable CPU offload for better performance
        sharding_strategy: "FULL_SHARD"  # Full sharding for maximum memory efficiency
        backward_prefetch: "BACKWARD_PRE"  # Optimize backward pass
        forward_prefetch: true       # Enable forward prefetch for better performance
        limit_all_gathers: true      # Reduce communication overhead
        activation_checkpointing: true   # Enable to reduce memory usage
        use_orig_params: true  # Set to true for single GPU FSDP compatibility
    
    # Optimizer configuration
    optimizer:
      type: "AdamW"
      weight_decay: 0.01             # Weight decay for instruction tuning
      betas: [0.9, 0.999]            # Standard betas
      eps: 1e-8                      # Epsilon for numerical stability
    
    # Learning rate scheduler configuration
    scheduler:
      type: "CosineAnnealingLR"      # Cosine annealing for better convergence
      T_max: 10000                   # Total steps for cosine annealing
      eta_min: 1e-7                  # Minimum learning rate
      warmup_steps: 500              # Warmup steps
      interval: "step"
      frequency: 1
    
    # Dataset configuration for instruction tuning
    data:
      # Processing configuration for instruction tuning datasets
      processing:
        max_tokens_per_sample: 2048            # Maximum tokens per instruction (shorter for instruction tuning)
        min_tokens_per_sample: 50              # Minimum tokens per training sample
        overlap_tokens: 0                      # No overlap for instruction tuning
        shuffle_datasets: true                 # Shuffle datasets before mixing
        seed: 42                               # Random seed for reproducibility
        total_samples: 100000                  # Total samples for instruction tuning (smaller than pretraining)
        use_processed_datasets: false          # Use HuggingFace datasets directly for instruction tuning
        save_processed_datasets: false         # Don't save processed datasets for instruction tuning
        test_size_samples: false               # If true, use only 4096 samples for testing
      
      # Output configuration for ds_processing pipeline
      output:
        processed_data_dir: "data/processed"   # Directory for processed HuggingFace datasets
        cache_dir: "/tmp/hf_cache"             # Use temporary cache directory
        save_format: "hf"                      # HuggingFace format for processed data
      
      # Stage 1 Instruction SFT: Instruction tuning datasets
      pretraining_datasets:
        # General instruction tuning datasets
        "allenai/tulu-v2-sft-mixture":
          percentage: 20.0                     # Tulu V2 SFT mixture - comprehensive instruction dataset
          subset: null                         # Full dataset
        
        "HuggingFaceTB/OpenHermes-2.5-H4":
          percentage: 20.0                     # OpenHermes 2.5 - high-quality instruction dataset
          subset: null                         # Full dataset
        
        # Math-focused datasets
        "nvidia/OpenMathInstruct-1":
          percentage: 15.0                     # NVIDIA OpenMathInstruct - math instruction dataset
          subset: null                         # Full dataset
        
        "meta-math/MetaMathQA":
          percentage: 15.0                     # MetaMathQA - math question answering dataset
          subset: null                         # Full dataset
        
        # Code-focused datasets
        "m-a-p/CodeFeedback-Filtered-Instruction":
          percentage: 10.0                     # CodeFeedback - filtered code instruction dataset
          subset: null                         # Full dataset
        
        # HPC-focused datasets
        "hpcgroup/hpc-instruct":
          percentage: 10.0                     # HPC Instruct - HPC instruction dataset
          subset: null                         # Full dataset
        
        "HPC-GPT/HPC":
          percentage: 10.0                     # HPC-GPT HPC dataset
          subset: null                         # Full dataset
          
        # Total: 100.0% (ensures proper allocation)
      
      # Data loading performance options (optimized for GH200 with 72 CPU cores)
      num_workers: 8                # Reduced from 16 to minimize overhead and contention
      pin_memory: true              # Pin memory for faster GPU transfer
      persistent_workers: true      # Keep workers alive between epochs
      prefetch_factor: 2            # Reduced from 4 to minimize memory overhead

    checkpoint_dir: "/home/sureshm/nemoACCfiy/outputs/checkpoints/stage1_inst_SFT"
    model_output_dir: "/home/sureshm/nemoACCfiy/outputs/models"
    model_output_name: "stage1_inst_SFT_model.pth"

    wandb:
      use_wandb: false
      project: "accfiy-stage1-inst-sft"
      entity: null
      run_name: null
      tags: ["stage1_inst_sft", "instruction_tuning", "decoder_only"]
      group: null
      notes: "Stage 1 Instruction SFT: Decoder-only instruction fine-tuning after NTP training"

    logging:
      debug: false
      log_file: null
      enable_profiling: true
      profiler_log_dir: "./profiler_logs/stage1_inst_SFT"
      save_steps: 1000
      validation_steps: 500
    
    # Environment and reproducibility
    environment:
      seed: 42                      # Random seed for reproducibility
      deterministic: false          # Deterministic training (slower but reproducible)
      benchmark: true               # CUDNN benchmark for better performance

  # Stage 2: Fine-tuning (Full Modular Model - comes later)
  stage2:
    model:
      model_config_key: "model_config_1.8B"
      tokenizer_path: "Qwen/Qwen3-Coder-30B-A3B-Instruct"
      use_flash_attention: true
      disable_flash_attention: false
      
      # Stage 1 checkpoint loading for Stage 2 training
      stage1_checkpoint_path: null  # Path to Stage 1 checkpoint (null = auto-detect)
      freeze_embedder_decoder: true # Freeze embedder decoder from Stage 1
      tie_weights: false            # Tie embedding and LM head weights

    training:
      # Training backend selection (for production mode)
      training_backend: "lightning"  # Options: "lightning" (PyTorch Lightning) or "megatron" (NeMo Megatron)
      
      # Stage 2 training parameters optimized for instruction tuning
      epochs: 3                       # More epochs for fine-tuning
      batch_size: 8                   # Smaller batch size for instruction tuning
      sequence_length: 1024           # Decoder sequence length
      embed_sequence_length: 4096     # Embedder sequence length (reasoning)
      learning_rate: 5e-6             # Lower learning rate for fine-tuning
      max_grad_norm: 0.8              # Gradient clipping (aligned with Stage 1)
      gradient_accumulation_steps: 2  # Gradient accumulation for effective larger batch size
      gradient_checkpointing: true    # Enable for memory efficiency
      mixed_precision: "bf16-mixed"   # Mixed precision training
      
      # Dual loss configuration
      contrastive_weight: 0.1         # Weight for contrastive loss (embedder)
      cross_entropy_weight: 1.0       # Weight for cross-entropy loss (decoder)
    
    # Distributed training configuration (aligned with Stage 1)
    distributed:
      strategy: "fsdp"  # Use FSDP for consistency with Stage 1
      num_nodes: 1
      devices: "auto"  # Can be "auto", number, or list of device IDs
      sync_batchnorm: false
      
      # FSDP specific configuration (aligned with Stage 1)
      fsdp:
        enabled: true                 # Enable FSDP for memory efficiency (aligned with Stage 1)
        cpu_offload: false           # Disable CPU offload for better performance
        sharding_strategy: "FULL_SHARD"  # Full sharding for maximum memory efficiency
        backward_prefetch: "BACKWARD_PRE"  # Optimize backward pass
        forward_prefetch: true       # Enable forward prefetch for better performance
        limit_all_gathers: true      # Reduce communication overhead
        activation_checkpointing: true   # Re-enable to reduce memory usage
        use_orig_params: true        # Set to true for single GPU FSDP compatibility
    
    # Optimizer configuration (aligned with Stage 1 for consistency)
    optimizer:
      type: "AdamW"
      weight_decay: 0.01              # Keep Stage 2 specific value for fine-tuning
      betas: [0.9, 0.95]              # More stable betas for fine-tuning
      eps: 1e-8                       # Keep Stage 2 specific value for fine-tuning
    
    # Learning rate scheduler configuration (optimized for instruction tuning)
    scheduler:
      type: "CosineAnnealingLR"       # Cosine annealing for better convergence
      T_max: 10000                    # Total steps for cosine annealing
      eta_min: 1e-7                   # Minimum learning rate
      warmup_steps: 500               # Warmup steps
      interval: "step"
      frequency: 1
    
    # Dataset configuration (NeMo HFDatasetDataModule with ds_processing pipeline)
    data:
      # Processing configuration for instruction tuning datasets
      processing:
        max_tokens_per_sample: 2048            # Maximum tokens per instruction (shorter for instruction tuning)
        min_tokens_per_sample: 50              # Minimum tokens per training sample
        overlap_tokens: 0                      # No overlap for instruction tuning
        shuffle_datasets: true                 # Shuffle datasets before mixing
        seed: 42                               # Random seed for reproducibility
        total_samples: 100000                  # Total samples for instruction tuning (smaller than pretraining)
        use_processed_datasets: true           # Set to true to use preprocessed datasets (much faster!)
        save_processed_datasets: true          # Save processed datasets for future use
        test_size_samples: false               # If true, use only 4096 samples for testing
      
      # Output configuration for ds_processing pipeline
      output:
        processed_data_dir: "data/processed"   # Directory for processed HuggingFace datasets
        cache_dir: "/tmp/hf_cache"             # Use temporary cache directory
        save_format: "hf"                      # HuggingFace format for processed data
      
      # Stage 2: Fine-tuning datasets (Instruction Tuning)
      pretraining_datasets:
        # Instruction tuning datasets with percentage allocation
        "nvidia/OpenCodeReasoning":
          percentage: 50.0                     # Code reasoning dataset
          subset: null                         # Full dataset
        
        "nvidia/OpenMathReasoning":
          percentage: 50.0                     # Math reasoning dataset
          subset: null                         # Full dataset
        
        # Total: 100.0% (ensures proper allocation)
      
      # Data loading performance options (optimized for GH200 with 72 CPU cores)
      num_workers: 8                # Reduced from 16 to minimize overhead and contention
      pin_memory: true              # Pin memory for faster GPU transfer
      persistent_workers: true      # Keep workers alive between epochs
      prefetch_factor: 2            # Reduced from 4 to minimize memory overhead

    checkpoint_dir: "/home/sureshm/nemoACCfiy/outputs/checkpoints/stage2"
    model_output_dir: "/home/sureshm/nemoACCfiy/outputs/models"
    model_output_name: "stage2_model.pth"

    # Stage 2 specific training monitoring
    checkpointing:
      save_every_n_steps: 1000        # Save checkpoints every 1000 steps
      save_top_k: 3                   # Keep top 3 checkpoints
      monitor: "val_loss"             # Metric to monitor for best checkpoints
      mode: "min"                     # Monitor mode (min/max)
      filename: "stage2_checkpoint-{step:06d}-{val_loss:.4f}"
      auto_insert_metric_name: false
      
      # Resume training settings
      resume_from_checkpoint: null    # Path to checkpoint to resume from
      resume_ignore_mismatched_sizes: true

    # Training monitoring and control (aligned with Stage 1)
    log_every_n_steps: 100           # Log every 100 steps (aligned with Stage 1)
    val_check_interval_steps: 1000   # Validate every 1000 steps (more frequent than Stage 1 for fine-tuning)
    
    # Early stopping
    patience: 5                      # Early stopping patience
    
    # Gradient optimization (aligned with Stage 1)
    gradient_clip_val: 0.8           # Gradient clipping value (aligned with Stage 1)
    gradient_clip_algorithm: "value" # Gradient clipping algorithm (aligned with Stage 1)

    wandb:
      use_wandb: false
      project: "accfiy-stage2"
      entity: null
      run_name: null
      tags: ["stage2", "instruction_tuning", "contrastive_loss"]
      group: null
      notes: "Stage 2 training with instruction tuning and dual loss functions"

    logging:
      debug: false
      log_file: null
      enable_profiling: true
      profiler_log_dir: "./profiler_logs/stage2"
      save_steps: 1000
      validation_steps: 500
    
    # Environment and reproducibility (aligned with Stage 1)
    environment:
      seed: 42                      # Random seed for reproducibility
      deterministic: false          # Deterministic training (slower but reproducible)
      benchmark: true               # CUDNN benchmark for better performance


# Global Configuration
global_config:
  # Default tokenizer path
  default_tokenizer_path: "Qwen/Qwen3-Coder-30B-A3B-Instruct"
  
  # Default output directories
  default_checkpoint_dir: "outputs/checkpoints"
  default_model_output_dir: "outputs/models"
  default_log_dir: "outputs/logs"
  
  # Default training parameters
  default_batch_size: 8
  default_learning_rate: 1e-5
  default_max_epochs: 3
  default_mixed_precision: "bf16"
  
  # Default wandb settings
  default_wandb_project: "accfiy"
  default_wandb_entity: null

# Example distributed training configurations
distributed_examples:
  # Single node, multiple GPUs with FSDP
  single_node_fsdp:
    strategy: "fsdp"
    num_nodes: 1
    devices: 1  # 8 GPUs on single node
    fsdp:
      enabled: true
      cpu_offload: false
      sharding_strategy: "FULL_SHARD"
      activation_checkpointing: true
  
  # Multi-node, multiple GPUs with FSDP
  multi_node_fsdp:
    strategy: "fsdp"
    num_nodes: 4  # 4 nodes
    devices: 8    # 8 GPUs per node
    fsdp:
      enabled: true
      cpu_offload: true  # Enable CPU offload for very large models
      sharding_strategy: "FULL_SHARD"
      activation_checkpointing: true
  
  # Multi-node with DDP (alternative to FSDP)
  multi_node_ddp:
    strategy: "ddp"
    num_nodes: 4
    devices: 8
    sync_batchnorm: true
  
  # Large scale training with CPU offload
  large_scale_fsdp:
    strategy: "fsdp"
    num_nodes: 8
    devices: 8
    fsdp:
      enabled: true
      cpu_offload: true
      sharding_strategy: "FULL_SHARD"
      backward_prefetch: "BACKWARD_PRE"
      forward_prefetch: true
      limit_all_gathers: true
      activation_checkpointing: true
      use_orig_params: false

