# Unified Configuration for ModularModel
# This file combines model architectures and training configurations

# Model Architecture Configurations
model_configs:
  model_config_32B:
    decoder_config:
      hidden_size: 5120
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 40960
      num_hidden_layers: 64
      num_attention_heads: 64
      intermediate_size: 25600
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_14B:
    decoder_config:
      hidden_size: 5120
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 40960
      num_hidden_layers: 40
      num_attention_heads: 40
      intermediate_size: 17408
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_8B:
    decoder_config:
      hidden_size: 4096
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 40960
      num_hidden_layers: 36
      num_attention_heads: 32
      intermediate_size: 12288
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-05
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_4B:
    decoder_config:
      attention_bias: false
      attention_dropout: 0.0
      hidden_act: "silu"
      hidden_size: 2560
      initializer_range: 0.02
      intermediate_size: 9728
      max_position_embeddings: 40960
      max_window_layers: 36
      num_attention_heads: 32
      num_hidden_layers: 36
      num_key_value_heads: 8
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_1.7B:
    decoder_config:
      attention_bias: false
      attention_dropout: 0.0
      hidden_act: "silu"
      hidden_size: 2048
      initializer_range: 0.02
      intermediate_size: 6144
      max_position_embeddings: 40960
      max_window_layers: 28
      num_attention_heads: 16
      num_hidden_layers: 28
      num_key_value_heads: 8
      RMSNorm_eps: 1e-06
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_tiny:
    decoder_config:
      hidden_size: 1024
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 512
      num_hidden_layers: 12
      num_attention_heads: 16
      intermediate_size: 1024
      num_key_value_heads: 8
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-05
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

  model_config_micro:
    decoder_config:
      hidden_size: 512
      pad_token_id: 0
      hidden_dropout_prob: 0.1
      attention_probs_dropout_prob: 0.1
      max_position_embeddings: 256
      num_hidden_layers: 6
      num_attention_heads: 8
      intermediate_size: 512
      num_key_value_heads: 4
      type_vocab_size: 2
      hidden_act: "gelu"
      initializer_range: 0.02
      RMSNorm_eps: 1e-05
      use_cache: true
    pool_type: "attention"
    hidden_dropout_prob: 0.0
    mlp_type: "mlp_gated"
    attention_type: "vanilla"

# Training Stage Configurations
training_stages:
  # Stage 1: Pre-training
  stage1:
    model:
      model_config_key: "model_config_1.7B"
      tokenizer_path: "tokenizers/qwen3-coder-30b-a3b-instruct-custom"
      use_flash_attention: true
      disable_flash_attention: false
      
      # Model architecture options (inherited from model_config_1.7B)
      tie_weights: true             # Tie embedding and LM head weights
      freeze_embedder_decoder: true # Freeze embedder decoder
      embedder_checkpoint_path: null # Path to embedder checkpoint

    training:
      # Training backend selection (for production mode)
      training_backend: "lightning"  # Options: "lightning" (PyTorch Lightning) or "megatron" (NeMo Megatron)
      
      # Training duration - epoch-based approach (automatically converted to steps internally)
      epochs: 1                     # Number of epochs to train
      batch_size: 8
      sequence_length: 2048
      learning_rate: 1e-6
      max_grad_norm: 1.5
      gradient_accumulation_steps: 2
      gradient_checkpointing: true
      mixed_precision: "bf16"
      
      # Step-based checkpointing and validation (more frequent monitoring)
      checkpointing:
        save_every_n_steps: 5000    # Save checkpoint every N steps (more frequent than epochs)
        save_top_k: 3               # Number of best checkpoints to keep
        monitor: "val_loss"         # Metric to monitor for best checkpoints
        mode: "min"                 # Monitor mode (min/max)
        filename: "checkpoint-{step:06d}-{val_loss:.4f}"  # Checkpoint filename (uses steps internally)
        auto_insert_metric_name: false  # Don't insert metric name in filename
        
        # Resume training settings
        resume_from_checkpoint: null  # Path to checkpoint to resume from (null = auto-detect)
        resume_ignore_mismatched_sizes: false  # Ignore size mismatches when resuming
        
      # Training monitoring and control
      log_every_n_steps: 10         # Log frequency (in steps)
      val_check_interval_steps: 5000 # Validation every N steps (more frequent than epochs)
      
      # Early stopping
      patience: 3                   # Early stopping patience (in epochs) - reasonable for epoch-based training
      
      # Gradient optimization
      gradient_clip_val: 1.0        # Gradient clipping value
      gradient_clip_algorithm: "norm"  # Gradient clipping algorithm
    
    # Distributed training configuration
    distributed:
      strategy: "auto"  # Options: "auto", "ddp", "fsdp", "deepspeed"
      num_nodes: 1
      devices: "auto"  # Can be "auto", number, or list of device IDs
      sync_batchnorm: false
      
      # FSDP specific configuration
      fsdp:
        enabled: false
        cpu_offload: false
        sharding_strategy: "FULL_SHARD"  # Options: "FULL_SHARD", "SHARD_GRAD_OP", "NO_SHARD"
        backward_prefetch: "BACKWARD_PRE"  # Options: "BACKWARD_PRE", "BACKWARD_POST", "NONE"
        forward_prefetch: false
        limit_all_gathers: true
        activation_checkpointing: true
        use_orig_params: false
    
    # Optimizer configuration
    optimizer:
      type: "AdamW"
      weight_decay: 0.01
      betas: [0.9, 0.999]
      eps: 1e-8
    
    # Learning rate scheduler configuration
    scheduler:
      type: "LinearLR"
      start_factor: 0.1
      end_factor: 1.0
      warmup_steps: 1000
      interval: "step"
      frequency: 1
    
    # Dataset configuration
    data:
      # Use HuggingFace datasets from the datasets section below
      data_path: null  # Set to null to use HuggingFace datasets
      max_samples: 10000  # Limit samples for testing (remove for full training)
      
      # Data loading performance options
      num_workers: 8                # Number of DataLoader workers
      pin_memory: true              # Pin memory for faster GPU transfer
      persistent_workers: true      # Keep workers alive between epochs
      prefetch_factor: 2            # Prefetch batches for better performance

    checkpoint_dir: "outputs/checkpoints/stage1"
    model_output_dir: "outputs/models"
    model_output_name: "stage1_model.pth"

    wandb:
      use_wandb: false
      project: "accfiy-stage1"
      entity: null
      run_name: null
      tags: ["stage1", "pretraining"]
      group: null
      notes: null

    logging:
      debug: false
      log_file: null
      enable_profiling: true
      profiler_log_dir: "./profiler_logs/stage1"
      save_steps: 1000
      validation_steps: 1000
    
    # Environment and reproducibility
    environment:
      seed: 42                      # Random seed for reproducibility
      deterministic: false          # Deterministic training (slower but reproducible)
      benchmark: true               # CUDNN benchmark for better performance

  # Stage 2: Fine-tuning
  stage2:
    model:
      model_config_key: "model_config_1.7B"
      tokenizer_path: "tokenizers/qwen3-coder-30b-a3b-instruct-custom"
      use_flash_attention: true
      disable_flash_attention: false

    training:
      # Training backend selection (for production mode)
      training_backend: "lightning"  # Options: "lightning" (PyTorch Lightning) or "megatron" (NeMo Megatron)
      
      epochs: 2
      batch_size: 16
      sequence_length: 1024
      embed_sequence_length: 4096
      learning_rate: 5e-6
      max_grad_norm: 2.5
      gradient_accumulation_steps: 1
      gradient_checkpointing: false
      mixed_precision: "bf16"
    
    # Distributed training configuration
    distributed:
      strategy: "auto"  # Options: "auto", "ddp", "fsdp", "deepspeed"
      num_nodes: 1
      devices: "auto"  # Can be "auto", number, or list of device IDs
      sync_batchnorm: false
      
      # FSDP specific configuration
      fsdp:
        enabled: false
        cpu_offload: false
        sharding_strategy: "FULL_SHARD"  # Options: "FULL_SHARD", "SHARD_GRAD_OP", "NO_SHARD"
        backward_prefetch: "BACKWARD_PRE"  # Options: "BACKWARD_PRE", "BACKWARD_POST", "NONE"
        forward_prefetch: false
        limit_all_gathers: true
        activation_checkpointing: true
        use_orig_params: false
    
    # Optimizer configuration
    optimizer:
      type: "AdamW"
      weight_decay: 0.01
      betas: [0.9, 0.999]
      eps: 1e-8
    
    # Learning rate scheduler configuration
    scheduler:
      type: "LinearLR"
      start_factor: 0.1
      end_factor: 1.0
      warmup_steps: 1000
      interval: "step"
      frequency: 1
    
    # Dataset configuration
    data:
      data_path: ""  # e.g., "data/fine_tuning_data.jsonl"
      max_samples: null

    checkpoint_dir: "outputs/checkpoints/stage2"
    model_output_dir: "outputs/models"
    model_output_name: "stage2_model.pth"

    wandb:
      use_wandb: false
      project: "accfiy-stage2"
      entity: null
      run_name: null
      tags: ["stage2", "finetuning"]
      group: null
      notes: null

    logging:
      debug: false
      log_file: null
      enable_profiling: true
      profiler_log_dir: "./profiler_logs/stage2"
      save_steps: 1000
      validation_steps: 1000


# Global Configuration
global_config:
  # Default tokenizer path
  default_tokenizer_path: "tokenizers/qwen3-coder-30b-a3b-instruct-custom"
  
  # Default output directories
  default_checkpoint_dir: "outputs/checkpoints"
  default_model_output_dir: "outputs/models"
  default_log_dir: "outputs/logs"
  
  # Default training parameters
  default_batch_size: 8
  default_learning_rate: 1e-5
  default_max_epochs: 3
  default_mixed_precision: "bf16"
  
  # Default wandb settings
  default_wandb_project: "accfiy"
  default_wandb_entity: null

# Example distributed training configurations
distributed_examples:
  # Single node, multiple GPUs with FSDP
  single_node_fsdp:
    strategy: "fsdp"
    num_nodes: 1
    devices: 8  # 8 GPUs on single node
    fsdp:
      enabled: true
      cpu_offload: false
      sharding_strategy: "FULL_SHARD"
      activation_checkpointing: true
  
  # Multi-node, multiple GPUs with FSDP
  multi_node_fsdp:
    strategy: "fsdp"
    num_nodes: 4  # 4 nodes
    devices: 8    # 8 GPUs per node
    fsdp:
      enabled: true
      cpu_offload: true  # Enable CPU offload for very large models
      sharding_strategy: "FULL_SHARD"
      activation_checkpointing: true
  
  # Multi-node with DDP (alternative to FSDP)
  multi_node_ddp:
    strategy: "ddp"
    num_nodes: 4
    devices: 8
    sync_batchnorm: true
  
  # Large scale training with CPU offload
  large_scale_fsdp:
    strategy: "fsdp"
    num_nodes: 8
    devices: 8
    fsdp:
      enabled: true
      cpu_offload: true
      sharding_strategy: "FULL_SHARD"
      backward_prefetch: "BACKWARD_PRE"
      forward_prefetch: true
      limit_all_gathers: true
      activation_checkpointing: true
      use_orig_params: false

# Global Dataset Configuration
datasets:
  # Global dataset processing settings (inherited by all stages)
  global_settings:
    # Dataset loading strategy (FIXED: now properly handles streaming for all datasets)
    use_streaming: true                    # Use streaming for efficient loading
    streaming_fallback: true               # Fallback to full download if streaming fails
    default_streaming_samples: 1000        # Default samples when streaming without limit
    cache_datasets: true                   # Cache downloaded datasets locally
    
    # Note: Streaming now works correctly for mlfoundations/dclm-baseline-1.0 and other datasets
    
    # Output settings
    processed_data_dir: "data/processed"   # Directory for processed datasets
    cache_dir: "~/.cache/huggingface"      # Use HuggingFace default cache directory
    save_format: "jsonl"                   # Format for saved processed data
    
    # Common processing settings
    shuffle_datasets: true                 # Shuffle datasets before mixing
    seed: 42                               # Random seed for reproducibility

  # Stage 1: Pre-training datasets (Next Token Prediction)
  stage1_datasets:
    # Hugging Face pretraining datasets with percentage allocation
    pretraining_datasets:
      # High-quality web content dataset
      "HuggingFaceFW/fineweb":
        percentage: 60.0                     # Increased from 40.0% (added 20% from bigcode)
        subset: "sample-10BT"               # 10 billion token sample
      
      # Code datasets (programming knowledge) - commented out due to S3 access requirements
      # "bigcode/the-stack-v2-dedup":
      #   percentage: 30.0
      #   subsets: ["C", "C++", "Python", "Cuda"]  # C, C++, Python, CUDA subsets
      
      # Math and reasoning datasets
      "HuggingFaceTB/finemath":
        percentage: 20.0                     # Increased from 15.0% (added 5% from bigcode)
        subset: "finemath-3plus"             # Math content subset
      
      # Curated content (FIXED: streaming now works correctly)
      "mlfoundations/dclm-baseline-1.0":
        percentage: 15.0                     # Increased from 10.0% (added 5% from bigcode)
        subset: null                         # 187k tokens - Curated content (streaming works)
      
      # Synthetic educational content
      "HuggingFaceTB/cosmopedia":
        percentage: 5.0                      # Kept same
        subset: "auto_math_text"             # Synthetic textbooks subset
      
      # Total: 100.0% (ensures proper allocation)
    
    # Stage 1 specific processing settings
    processing:
      max_tokens_per_sample: 2048            # Maximum tokens per training sample
      min_tokens_per_sample: 50              # Minimum tokens per training sample
      overlap_tokens: 100                    # Overlap between consecutive samples for better training
      max_samples_per_dataset: 10000         # Limit samples per dataset for faster testing

  # Stage 2: Fine-tuning datasets (Task-specific training)
  stage2_datasets:
    # Fine-tuning datasets for specific tasks
    finetuning_datasets:
      # Code completion and generation - commented out due to S3 access requirements
      # "bigcode/the-stack-v2-dedup":
      #   percentage: 50.0
      #   subsets: ["C", "C++", "Python", "Cuda"]  # C, C++, Python, CUDA subsets
      #   task_type: "code_completion"
      
      # Mathematical reasoning
      "HuggingFaceTB/finemath":
        percentage: 60.0                     # Increased from 30.0% (added 30% from bigcode)
        subset: "finemath-3plus"             # Math content subset
        task_type: "mathematical_reasoning"
      
      # General knowledge and reasoning
      "HuggingFaceFW/fineweb":
        percentage: 40.0                     # Increased from 20.0% (added 20% from bigcode)
        subset: "sample-10BT"               # 10 billion token sample
        task_type: "general_knowledge"
    
    # Stage 2 specific processing settings
    processing:
      max_tokens_per_sample: 1024            # Shorter sequences for fine-tuning
      min_tokens_per_sample: 100             # Longer minimum for task-specific content
      overlap_tokens: 50                     # Less overlap for fine-tuning
      max_samples_per_dataset: 10000         # Limit for fine-tuning datasets (reduced for testing)
      task_specific_processing: true         # Enable task-specific data processing

  # Stage 3: Instruction tuning datasets (Conversational AI)
  stage3_datasets:
    # Instruction tuning datasets for conversational AI
    instruction_datasets:
      # High-quality instruction datasets
      "HuggingFaceTB/cosmopedia":
        percentage: 50.0                     # Increased from 40.0% (added 10% from bigcode)
        subset: "auto_math_text"             # Educational content
        task_type: "instruction_following"
      
      # Code instruction datasets - commented out due to S3 access requirements
      # "bigcode/the-stack-v2-dedup":
      #   percentage: 30.0
      #   subsets: ["C", "C++", "Python", "Cuda"]  # C, C++, Python, CUDA subsets
      #   task_type: "code_instruction"
      
      # General instruction datasets
      "HuggingFaceFW/fineweb":
        percentage: 30.0                     # Increased from 20.0% (added 10% from bigcode)
        subset: "sample-10BT"               # 10 billion token sample
        task_type: "general_instruction"
      
      # Math instruction datasets
      "HuggingFaceTB/finemath":
        percentage: 20.0                     # Increased from 10.0% (added 10% from bigcode)
        subset: "finemath-3plus"             # Math instruction
        task_type: "math_instruction"
    
    # Stage 3 specific processing settings
    processing:
      max_tokens_per_sample: 4096            # Longer sequences for conversations
      min_tokens_per_sample: 200             # Longer minimum for instruction pairs
      overlap_tokens: 0                      # No overlap for instruction pairs
      max_samples_per_dataset: 10000         # More samples for instruction tuning (reduced for testing)
      instruction_format: "alpaca"           # Instruction format (alpaca, sharegpt, etc.)
      include_system_prompts: true           # Include system prompts
      conversation_format: true              # Enable conversation formatting
